{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn.linear_model as ln\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"X_data.csv\", index_col=\"Unnamed: 0\", parse_dates=True, sep = ';')\n",
    "df.index.names = [\"Date\"]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "                     T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\nDate                                                                  \n2015-01-01 00:00:00         212         210         211         347   \n2015-01-01 00:01:00         212         211         211         346   \n2015-01-01 00:02:00         212         211         211         345   \n2015-01-01 00:03:00         213         211         211         344   \n2015-01-01 00:04:00         213         211         211         343   \n2015-01-01 00:05:00         213         211         211         342   \n2015-01-01 00:06:00         213         212         211         341   \n\n                     T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  \\\nDate                                                                  \n2015-01-01 00:00:00         353         347         474         473   \n2015-01-01 00:01:00         352         346         475         473   \n2015-01-01 00:02:00         352         346         476         473   \n2015-01-01 00:03:00         351         346         477         473   \n2015-01-01 00:04:00         350         346         478         473   \n2015-01-01 00:05:00         350         346         479         473   \n2015-01-01 00:06:00         349         346         480         473   \n\n                     T_data_3_3  T_data_4_1  T_data_4_2  T_data_4_3  \\\nDate                                                                  \n2015-01-01 00:00:00         481         346         348         355   \n2015-01-01 00:01:00         481         349         348         355   \n2015-01-01 00:02:00         481         352         349         355   \n2015-01-01 00:03:00         481         355         349         355   \n2015-01-01 00:04:00         482         358         349         355   \n2015-01-01 00:05:00         482         360         349         354   \n2015-01-01 00:06:00         482         363         350         354   \n\n                     T_data_5_1  T_data_5_2  T_data_5_3  H_data  AH_data  \nDate                                                                      \n2015-01-01 00:00:00         241         241         243  167.85     9.22  \n2015-01-01 00:01:00         241         241         243  162.51     9.22  \n2015-01-01 00:02:00         242         241         242  164.99     9.22  \n2015-01-01 00:03:00         242         241         242  167.34     9.22  \n2015-01-01 00:04:00         243         241         242  163.04     9.22  \n2015-01-01 00:05:00         243         241         242  163.81     9.22  \n2015-01-01 00:06:00         244         241         242  166.14     9.22  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T_data_1_1</th>\n      <th>T_data_1_2</th>\n      <th>T_data_1_3</th>\n      <th>T_data_2_1</th>\n      <th>T_data_2_2</th>\n      <th>T_data_2_3</th>\n      <th>T_data_3_1</th>\n      <th>T_data_3_2</th>\n      <th>T_data_3_3</th>\n      <th>T_data_4_1</th>\n      <th>T_data_4_2</th>\n      <th>T_data_4_3</th>\n      <th>T_data_5_1</th>\n      <th>T_data_5_2</th>\n      <th>T_data_5_3</th>\n      <th>H_data</th>\n      <th>AH_data</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-01-01 00:00:00</th>\n      <td>212</td>\n      <td>210</td>\n      <td>211</td>\n      <td>347</td>\n      <td>353</td>\n      <td>347</td>\n      <td>474</td>\n      <td>473</td>\n      <td>481</td>\n      <td>346</td>\n      <td>348</td>\n      <td>355</td>\n      <td>241</td>\n      <td>241</td>\n      <td>243</td>\n      <td>167.85</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:01:00</th>\n      <td>212</td>\n      <td>211</td>\n      <td>211</td>\n      <td>346</td>\n      <td>352</td>\n      <td>346</td>\n      <td>475</td>\n      <td>473</td>\n      <td>481</td>\n      <td>349</td>\n      <td>348</td>\n      <td>355</td>\n      <td>241</td>\n      <td>241</td>\n      <td>243</td>\n      <td>162.51</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:02:00</th>\n      <td>212</td>\n      <td>211</td>\n      <td>211</td>\n      <td>345</td>\n      <td>352</td>\n      <td>346</td>\n      <td>476</td>\n      <td>473</td>\n      <td>481</td>\n      <td>352</td>\n      <td>349</td>\n      <td>355</td>\n      <td>242</td>\n      <td>241</td>\n      <td>242</td>\n      <td>164.99</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:03:00</th>\n      <td>213</td>\n      <td>211</td>\n      <td>211</td>\n      <td>344</td>\n      <td>351</td>\n      <td>346</td>\n      <td>477</td>\n      <td>473</td>\n      <td>481</td>\n      <td>355</td>\n      <td>349</td>\n      <td>355</td>\n      <td>242</td>\n      <td>241</td>\n      <td>242</td>\n      <td>167.34</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:04:00</th>\n      <td>213</td>\n      <td>211</td>\n      <td>211</td>\n      <td>343</td>\n      <td>350</td>\n      <td>346</td>\n      <td>478</td>\n      <td>473</td>\n      <td>482</td>\n      <td>358</td>\n      <td>349</td>\n      <td>355</td>\n      <td>243</td>\n      <td>241</td>\n      <td>242</td>\n      <td>163.04</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:05:00</th>\n      <td>213</td>\n      <td>211</td>\n      <td>211</td>\n      <td>342</td>\n      <td>350</td>\n      <td>346</td>\n      <td>479</td>\n      <td>473</td>\n      <td>482</td>\n      <td>360</td>\n      <td>349</td>\n      <td>354</td>\n      <td>243</td>\n      <td>241</td>\n      <td>242</td>\n      <td>163.81</td>\n      <td>9.22</td>\n    </tr>\n    <tr>\n      <th>2015-01-01 00:06:00</th>\n      <td>213</td>\n      <td>212</td>\n      <td>211</td>\n      <td>341</td>\n      <td>349</td>\n      <td>346</td>\n      <td>480</td>\n      <td>473</td>\n      <td>482</td>\n      <td>363</td>\n      <td>350</td>\n      <td>354</td>\n      <td>244</td>\n      <td>241</td>\n      <td>242</td>\n      <td>166.14</td>\n      <td>9.22</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(7)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "надо бы разбить данные на тренировочные и тестовые"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "                     T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\nDate                                                                  \n2015-01-03 23:05:00         266         339         263         335   \n2015-01-03 23:06:00         266         339         263         334   \n2015-01-03 23:07:00         267         339         263         334   \n2015-01-03 23:08:00         267         339         263         334   \n2015-01-03 23:09:00         267         340         263         334   \n...                         ...         ...         ...         ...   \n2015-01-04 00:10:00         277         322         273         321   \n2015-01-04 00:11:00         277         322         273         321   \n2015-01-04 00:12:00         277         321         274         321   \n2015-01-04 00:13:00         277         320         274         321   \n2015-01-04 00:14:00         277         319         274         321   \n\n                     T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  \\\nDate                                                                  \n2015-01-03 23:05:00         326         341         495         497   \n2015-01-03 23:06:00         326         341         496         498   \n2015-01-03 23:07:00         326         341         496         498   \n2015-01-03 23:08:00         326         341         497         498   \n2015-01-03 23:09:00         326         342         497         498   \n...                         ...         ...         ...         ...   \n2015-01-04 00:10:00         334         353         504         501   \n2015-01-04 00:11:00         334         353         504         501   \n2015-01-04 00:12:00         334         354         504         501   \n2015-01-04 00:13:00         334         354         504         501   \n2015-01-04 00:14:00         334         354         504         501   \n\n                     T_data_3_3  T_data_4_1  T_data_4_2  T_data_4_3  \\\nDate                                                                  \n2015-01-03 23:05:00         535         316         346         336   \n2015-01-03 23:06:00         537         317         347         336   \n2015-01-03 23:07:00         538         317         348         336   \n2015-01-03 23:08:00         540         317         348         336   \n2015-01-03 23:09:00         542         317         349         336   \n...                         ...         ...         ...         ...   \n2015-01-04 00:10:00         684         327         381         337   \n2015-01-04 00:11:00         687         327         381         337   \n2015-01-04 00:12:00         690         327         382         337   \n2015-01-04 00:13:00         692         327         382         337   \n2015-01-04 00:14:00         695         327         382         337   \n\n                     T_data_5_1  T_data_5_2  T_data_5_3  H_data  AH_data  \nDate                                                                      \n2015-01-03 23:05:00         234         238         239  154.87     4.73  \n2015-01-03 23:06:00         234         238         239  155.06     4.73  \n2015-01-03 23:07:00         233         238         239  155.19     4.73  \n2015-01-03 23:08:00         233         238         239  154.35     4.73  \n2015-01-03 23:09:00         233         238         240  153.78     4.73  \n...                         ...         ...         ...     ...      ...  \n2015-01-04 00:10:00         231         236         241  159.23     7.90  \n2015-01-04 00:11:00         231         236         241  155.84     7.90  \n2015-01-04 00:12:00         231         236         241  155.62     7.90  \n2015-01-04 00:13:00         231         236         241  157.55     7.90  \n2015-01-04 00:14:00         231         236         241  154.84     7.90  \n\n[70 rows x 17 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T_data_1_1</th>\n      <th>T_data_1_2</th>\n      <th>T_data_1_3</th>\n      <th>T_data_2_1</th>\n      <th>T_data_2_2</th>\n      <th>T_data_2_3</th>\n      <th>T_data_3_1</th>\n      <th>T_data_3_2</th>\n      <th>T_data_3_3</th>\n      <th>T_data_4_1</th>\n      <th>T_data_4_2</th>\n      <th>T_data_4_3</th>\n      <th>T_data_5_1</th>\n      <th>T_data_5_2</th>\n      <th>T_data_5_3</th>\n      <th>H_data</th>\n      <th>AH_data</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-01-03 23:05:00</th>\n      <td>266</td>\n      <td>339</td>\n      <td>263</td>\n      <td>335</td>\n      <td>326</td>\n      <td>341</td>\n      <td>495</td>\n      <td>497</td>\n      <td>535</td>\n      <td>316</td>\n      <td>346</td>\n      <td>336</td>\n      <td>234</td>\n      <td>238</td>\n      <td>239</td>\n      <td>154.87</td>\n      <td>4.73</td>\n    </tr>\n    <tr>\n      <th>2015-01-03 23:06:00</th>\n      <td>266</td>\n      <td>339</td>\n      <td>263</td>\n      <td>334</td>\n      <td>326</td>\n      <td>341</td>\n      <td>496</td>\n      <td>498</td>\n      <td>537</td>\n      <td>317</td>\n      <td>347</td>\n      <td>336</td>\n      <td>234</td>\n      <td>238</td>\n      <td>239</td>\n      <td>155.06</td>\n      <td>4.73</td>\n    </tr>\n    <tr>\n      <th>2015-01-03 23:07:00</th>\n      <td>267</td>\n      <td>339</td>\n      <td>263</td>\n      <td>334</td>\n      <td>326</td>\n      <td>341</td>\n      <td>496</td>\n      <td>498</td>\n      <td>538</td>\n      <td>317</td>\n      <td>348</td>\n      <td>336</td>\n      <td>233</td>\n      <td>238</td>\n      <td>239</td>\n      <td>155.19</td>\n      <td>4.73</td>\n    </tr>\n    <tr>\n      <th>2015-01-03 23:08:00</th>\n      <td>267</td>\n      <td>339</td>\n      <td>263</td>\n      <td>334</td>\n      <td>326</td>\n      <td>341</td>\n      <td>497</td>\n      <td>498</td>\n      <td>540</td>\n      <td>317</td>\n      <td>348</td>\n      <td>336</td>\n      <td>233</td>\n      <td>238</td>\n      <td>239</td>\n      <td>154.35</td>\n      <td>4.73</td>\n    </tr>\n    <tr>\n      <th>2015-01-03 23:09:00</th>\n      <td>267</td>\n      <td>340</td>\n      <td>263</td>\n      <td>334</td>\n      <td>326</td>\n      <td>342</td>\n      <td>497</td>\n      <td>498</td>\n      <td>542</td>\n      <td>317</td>\n      <td>349</td>\n      <td>336</td>\n      <td>233</td>\n      <td>238</td>\n      <td>240</td>\n      <td>153.78</td>\n      <td>4.73</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 00:10:00</th>\n      <td>277</td>\n      <td>322</td>\n      <td>273</td>\n      <td>321</td>\n      <td>334</td>\n      <td>353</td>\n      <td>504</td>\n      <td>501</td>\n      <td>684</td>\n      <td>327</td>\n      <td>381</td>\n      <td>337</td>\n      <td>231</td>\n      <td>236</td>\n      <td>241</td>\n      <td>159.23</td>\n      <td>7.90</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 00:11:00</th>\n      <td>277</td>\n      <td>322</td>\n      <td>273</td>\n      <td>321</td>\n      <td>334</td>\n      <td>353</td>\n      <td>504</td>\n      <td>501</td>\n      <td>687</td>\n      <td>327</td>\n      <td>381</td>\n      <td>337</td>\n      <td>231</td>\n      <td>236</td>\n      <td>241</td>\n      <td>155.84</td>\n      <td>7.90</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 00:12:00</th>\n      <td>277</td>\n      <td>321</td>\n      <td>274</td>\n      <td>321</td>\n      <td>334</td>\n      <td>354</td>\n      <td>504</td>\n      <td>501</td>\n      <td>690</td>\n      <td>327</td>\n      <td>382</td>\n      <td>337</td>\n      <td>231</td>\n      <td>236</td>\n      <td>241</td>\n      <td>155.62</td>\n      <td>7.90</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 00:13:00</th>\n      <td>277</td>\n      <td>320</td>\n      <td>274</td>\n      <td>321</td>\n      <td>334</td>\n      <td>354</td>\n      <td>504</td>\n      <td>501</td>\n      <td>692</td>\n      <td>327</td>\n      <td>382</td>\n      <td>337</td>\n      <td>231</td>\n      <td>236</td>\n      <td>241</td>\n      <td>157.55</td>\n      <td>7.90</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 00:14:00</th>\n      <td>277</td>\n      <td>319</td>\n      <td>274</td>\n      <td>321</td>\n      <td>334</td>\n      <td>354</td>\n      <td>504</td>\n      <td>501</td>\n      <td>695</td>\n      <td>327</td>\n      <td>382</td>\n      <td>337</td>\n      <td>231</td>\n      <td>236</td>\n      <td>241</td>\n      <td>154.84</td>\n      <td>7.90</td>\n    </tr>\n  </tbody>\n</table>\n<p>70 rows × 17 columns</p>\n</div>"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = df.loc[(df.index < \"2018-05-03 23:05:00\") & (df.index >= \"2015-01-03 23:05:00\")]\n",
    "X_train.head(70)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Заметим, что в тренировочных данных почему-то данные только с 4 января, а не с начала года. Поэтому отсечём тренировочные \"иксы\" начиная за час до начала тренировочных \"игреков\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "                     score\nDate                      \n2015-01-04 00:05:00    392\n2015-01-04 01:05:00    384\n2015-01-04 02:05:00    393\n2015-01-04 03:05:00    399\n2015-01-04 04:05:00    400",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2015-01-04 00:05:00</th>\n      <td>392</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 01:05:00</th>\n      <td>384</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 02:05:00</th>\n      <td>393</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 03:05:00</th>\n      <td>399</td>\n    </tr>\n    <tr>\n      <th>2015-01-04 04:05:00</th>\n      <td>400</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train = pd.read_csv(\"Y_train.csv\",names=[\"Date\", \"score\"], index_col=\"Date\", parse_dates=True, sep = ';')\n",
    "Y_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "score    264\ndtype: int64"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_train.nunique()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "И тут мы понимаем, что, вероятно, стоит писать нейронку. Ибо быстрый гуглёж показыват, что на таком кол-ве классов рандом форест чувствует себя не очень хорошо"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вроде бы сработало. Аналогично для сабмита:"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "                     T_data_1_1  T_data_1_2  T_data_1_3  T_data_2_1  \\\nDate                                                                  \n2018-12-31 22:45:00         271         265         260         357   \n2018-12-31 22:46:00         271         264         261         357   \n2018-12-31 22:47:00         271         263         261         357   \n2018-12-31 22:48:00         271         262         261         357   \n2018-12-31 22:49:00         271         261         261         357   \n2018-12-31 22:50:00         271         261         261         357   \n2018-12-31 22:51:00         271         260         261         357   \n2018-12-31 22:52:00         271         259         261         357   \n2018-12-31 22:53:00         271         258         261         357   \n2018-12-31 22:54:00         271         257         262         357   \n2018-12-31 22:55:00         271         256         262         357   \n2018-12-31 22:56:00         271         255         262         357   \n2018-12-31 22:57:00         271         254         262         357   \n2018-12-31 22:58:00         271         253         262         357   \n2018-12-31 22:59:00         271         252         262         357   \n2018-12-31 23:00:00         271         252         262         357   \n2018-12-31 23:01:00         271         251         262         357   \n2018-12-31 23:02:00         271         250         263         357   \n2018-12-31 23:03:00         271         249         263         357   \n2018-12-31 23:04:00         271         248         263         357   \n\n                     T_data_2_2  T_data_2_3  T_data_3_1  T_data_3_2  \\\nDate                                                                  \n2018-12-31 22:45:00         351         359         481         449   \n2018-12-31 22:46:00         355         359         481         449   \n2018-12-31 22:47:00         359         359         481         449   \n2018-12-31 22:48:00         359         359         481         449   \n2018-12-31 22:49:00         359         359         481         449   \n2018-12-31 22:50:00         359         359         481         449   \n2018-12-31 22:51:00         359         359         481         449   \n2018-12-31 22:52:00         359         359         481         449   \n2018-12-31 22:53:00         359         359         481         449   \n2018-12-31 22:54:00         359         360         481         449   \n2018-12-31 22:55:00         359         360         481         449   \n2018-12-31 22:56:00         359         360         481         449   \n2018-12-31 22:57:00         359         360         481         449   \n2018-12-31 22:58:00         359         360         481         449   \n2018-12-31 22:59:00         359         360         481         449   \n2018-12-31 23:00:00         359         360         481         449   \n2018-12-31 23:01:00         359         360         481         449   \n2018-12-31 23:02:00         359         360         481         449   \n2018-12-31 23:03:00         359         360         481         449   \n2018-12-31 23:04:00         359         360         481         449   \n\n                     T_data_3_3  T_data_4_1  T_data_4_2  T_data_4_3  \\\nDate                                                                  \n2018-12-31 22:45:00         491         325         331         327   \n2018-12-31 22:46:00         491         325         331         327   \n2018-12-31 22:47:00         491         325         331         327   \n2018-12-31 22:48:00         491         325         331         327   \n2018-12-31 22:49:00         491         325         331         327   \n2018-12-31 22:50:00         491         325         331         327   \n2018-12-31 22:51:00         491         325         331         327   \n2018-12-31 22:52:00         491         325         331         327   \n2018-12-31 22:53:00         491         325         332         327   \n2018-12-31 22:54:00         491         325         332         327   \n2018-12-31 22:55:00         491         325         332         327   \n2018-12-31 22:56:00         491         325         332         327   \n2018-12-31 22:57:00         491         325         332         328   \n2018-12-31 22:58:00         491         325         332         328   \n2018-12-31 22:59:00         491         325         332         328   \n2018-12-31 23:00:00         491         325         332         328   \n2018-12-31 23:01:00         491         325         332         328   \n2018-12-31 23:02:00         491         325         332         328   \n2018-12-31 23:03:00         491         325         332         328   \n2018-12-31 23:04:00         491         325         333         328   \n\n                     T_data_5_1  T_data_5_2  T_data_5_3  H_data  AH_data  \nDate                                                                      \n2018-12-31 22:45:00         277         276         280  157.82     6.13  \n2018-12-31 22:46:00         277         276         280  157.68     6.13  \n2018-12-31 22:47:00         277         276         280  159.87     6.13  \n2018-12-31 22:48:00         277         276         280  161.51     6.13  \n2018-12-31 22:49:00         277         276         280  159.33     6.13  \n2018-12-31 22:50:00         277         276         280  158.26     6.13  \n2018-12-31 22:51:00         277         276         280  161.93     6.13  \n2018-12-31 22:52:00         277         276         280  157.99     6.13  \n2018-12-31 22:53:00         277         276         280  161.65     6.13  \n2018-12-31 22:54:00         277         276         280  161.54     6.13  \n2018-12-31 22:55:00         277         276         280  154.92     6.13  \n2018-12-31 22:56:00         277         276         280  159.26     6.13  \n2018-12-31 22:57:00         277         276         280  161.05     6.13  \n2018-12-31 22:58:00         277         276         280  160.32     6.13  \n2018-12-31 22:59:00         277         276         280  158.21     6.13  \n2018-12-31 23:00:00         277         276         280  159.55     8.44  \n2018-12-31 23:01:00         277         276         280  159.78     8.44  \n2018-12-31 23:02:00         277         276         280  158.36     8.44  \n2018-12-31 23:03:00         277         276         280  159.77     8.44  \n2018-12-31 23:04:00         277         276         280  161.62     8.44  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T_data_1_1</th>\n      <th>T_data_1_2</th>\n      <th>T_data_1_3</th>\n      <th>T_data_2_1</th>\n      <th>T_data_2_2</th>\n      <th>T_data_2_3</th>\n      <th>T_data_3_1</th>\n      <th>T_data_3_2</th>\n      <th>T_data_3_3</th>\n      <th>T_data_4_1</th>\n      <th>T_data_4_2</th>\n      <th>T_data_4_3</th>\n      <th>T_data_5_1</th>\n      <th>T_data_5_2</th>\n      <th>T_data_5_3</th>\n      <th>H_data</th>\n      <th>AH_data</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-12-31 22:45:00</th>\n      <td>271</td>\n      <td>265</td>\n      <td>260</td>\n      <td>357</td>\n      <td>351</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>157.82</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:46:00</th>\n      <td>271</td>\n      <td>264</td>\n      <td>261</td>\n      <td>357</td>\n      <td>355</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>157.68</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:47:00</th>\n      <td>271</td>\n      <td>263</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.87</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:48:00</th>\n      <td>271</td>\n      <td>262</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.51</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:49:00</th>\n      <td>271</td>\n      <td>261</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.33</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:50:00</th>\n      <td>271</td>\n      <td>261</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>158.26</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:51:00</th>\n      <td>271</td>\n      <td>260</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.93</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:52:00</th>\n      <td>271</td>\n      <td>259</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>331</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>157.99</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:53:00</th>\n      <td>271</td>\n      <td>258</td>\n      <td>261</td>\n      <td>357</td>\n      <td>359</td>\n      <td>359</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.65</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:54:00</th>\n      <td>271</td>\n      <td>257</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.54</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:55:00</th>\n      <td>271</td>\n      <td>256</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>154.92</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:56:00</th>\n      <td>271</td>\n      <td>255</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>327</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.26</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:57:00</th>\n      <td>271</td>\n      <td>254</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.05</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:58:00</th>\n      <td>271</td>\n      <td>253</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>160.32</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:59:00</th>\n      <td>271</td>\n      <td>252</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>158.21</td>\n      <td>6.13</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:00:00</th>\n      <td>271</td>\n      <td>252</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.55</td>\n      <td>8.44</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:01:00</th>\n      <td>271</td>\n      <td>251</td>\n      <td>262</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.78</td>\n      <td>8.44</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:02:00</th>\n      <td>271</td>\n      <td>250</td>\n      <td>263</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>158.36</td>\n      <td>8.44</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:03:00</th>\n      <td>271</td>\n      <td>249</td>\n      <td>263</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>332</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>159.77</td>\n      <td>8.44</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:04:00</th>\n      <td>271</td>\n      <td>248</td>\n      <td>263</td>\n      <td>357</td>\n      <td>359</td>\n      <td>360</td>\n      <td>481</td>\n      <td>449</td>\n      <td>491</td>\n      <td>325</td>\n      <td>333</td>\n      <td>328</td>\n      <td>277</td>\n      <td>276</td>\n      <td>280</td>\n      <td>161.62</td>\n      <td>8.44</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_submit = df.loc[(df.index >= \"2018-05-03 23:05:00\") & (df.index < \"2018-12-31 23:05:00\")]\n",
    "X_submit.tail(20)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                     score\nDate                      \n2018-12-31 19:05:00    420\n2018-12-31 20:05:00    420\n2018-12-31 21:05:00    420\n2018-12-31 22:05:00    420\n2018-12-31 23:05:00    420",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-12-31 19:05:00</th>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 20:05:00</th>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 21:05:00</th>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 22:05:00</th>\n      <td>420</td>\n    </tr>\n    <tr>\n      <th>2018-12-31 23:05:00</th>\n      <td>420</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_submit= pd.read_csv(\"Y_submit.csv\",names=[\"Date\", \"score\"] ,index_col=\"Date\", parse_dates=True, sep = ';', )\n",
    "Y_submit.tail()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Препроцессинг данных почти завершён. Теперь закинем всё, что у нас есть в тензоры пайторча. Функцию для этого честно стащим со StackOverflow."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data.sampler import SubsetRandomSampler, Sampler\n",
    "# determine the supported device\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device('cuda:0')\n",
    "    else:\n",
    "        device = torch.device('cpu') # don't have GPU\n",
    "    return device\n",
    "\n",
    "# convert a df to tensor to be used in pytorch\n",
    "def df_to_tensor(df):\n",
    "    device = get_device()\n",
    "    return torch.from_numpy(df.values).float().to(device)\n",
    "\n",
    "#df_tensor = df_to_tensor(df)\n",
    "#print(df_tensor)\n",
    "print(get_device())\n",
    "device = get_device()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Данные будем хранить на видеокарте."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Закидываем в тензоры всё остальное"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "X_train_tensor  = df_to_tensor(X_train)\n",
    "X_submit_tensor = df_to_tensor(X_submit)\n",
    "Y_train_tensor = df_to_tensor(Y_train)\n",
    "Y_submit_tensor = df_to_tensor(Y_submit)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Будем считать, что марку определяет вся история данных за последний час. Тогда имеем 60*17 = 1020 чисел — размер вектора для нейронной сети."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29184.0\n",
      "29184\n",
      "5808.0\n",
      "5808\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_tensor) / 60)\n",
    "print(len(Y_train_tensor))\n",
    "print(len(X_submit_tensor) / 60)\n",
    "print(len(Y_submit_tensor))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Для самопроверки, мы ничего не потеряли. Раз мы сказали, что одно число из Y_train определяется 1020 числами из X_train, то разумно решейпнуть тензоры"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[266.0000, 339.0000, 263.0000,  ..., 242.0000, 155.4500,   7.9000],\n        [277.0000, 326.0000, 273.0000,  ..., 230.0000, 156.0000,   6.9600],\n        [277.0000, 253.0000, 272.0000,  ..., 240.0000, 154.1500,   7.2900],\n        ...,\n        [259.0000, 255.0000, 260.0000,  ..., 247.0000, 158.8700,   7.6500],\n        [260.0000, 254.0000, 263.0000,  ..., 248.0000, 158.6500,   8.9000],\n        [255.0000, 255.0000, 260.0000,  ..., 245.0000, 153.7400,   6.3900]],\n       device='cuda:0')"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_tensor = torch.reshape(X_train_tensor, (len(Y_train_tensor), 1020))\n",
    "X_train_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "сравнивая этот тензор с датафреймом выше, понимаем, что решейп прошёл успешно. делаем, аналогично для X_submit"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[254.0000, 257.0000, 256.0000,  ..., 236.0000, 157.0500,   6.8100],\n        [258.0000, 261.0000, 256.0000,  ..., 236.0000, 155.7000,   7.8900],\n        [255.0000, 263.0000, 258.0000,  ..., 255.0000, 156.8800,   6.1600],\n        ...,\n        [263.0000, 260.0000, 257.0000,  ..., 278.0000, 188.5700,   8.3700],\n        [268.0000, 276.0000, 255.0000,  ..., 280.0000, 158.5900,   6.1300],\n        [320.0000, 285.0000, 257.0000,  ..., 280.0000, 161.6200,   8.4400]],\n       device='cuda:0')"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_submit_tensor = torch.reshape(X_submit_tensor, (len(Y_submit_tensor), 1020))\n",
    "X_submit_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Вроде бы можно начинать писать саму нейронную сеть. Сделаем хорошое дело и создадим свой класс датасета под нашу задачу."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "class Dataset(torch.utils.data.Dataset):\n",
    "    'Characterizes a dataset for PyTorch'\n",
    "    def __init__(self, data, labels):\n",
    "        'Initialization'\n",
    "        self.labels = labels\n",
    "        self.list_IDs = list(range(len(labels)))\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the total number of samples'\n",
    "        return len(self.list_IDs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        'Generates one sample of data'\n",
    "        # Select sample\n",
    "        ID = self.list_IDs[index]\n",
    "\n",
    "        # Load data and get label\n",
    "        X = self.data[ID]\n",
    "        y = self.labels[ID]\n",
    "\n",
    "        return X, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "По факту следующая клетка кода не используется в финальном варианте исполнения. Она мне была нужна, когда я делал сплит по трейн даным для валидации. Но по итогу валидировался я уже на сабмите. Возможно спорное решение, но детали я опишу в отчёте."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "batch_size = 2048\n",
    "\n",
    "data_size = len(Y_train_tensor)\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "train_set = Dataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "submit_set = Dataset(X_submit_tensor, Y_submit_tensor)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "val_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "val_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                         sampler=val_sampler)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "попробуем без валидации на трэйн сете, мб это даст лучшие результаты"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "batch_size = 4096\n",
    "\n",
    "data_size = len(Y_train_tensor)\n",
    "validation_split = .2\n",
    "split = int(np.floor(validation_split * data_size))\n",
    "indices = list(range(data_size))\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "#train_indices, val_indices = indices[split:], indices[:split]\n",
    "train_indices = indices\n",
    "submit_indices = list(range(len(Y_submit_tensor)))\n",
    "np.random.shuffle(submit_indices)\n",
    "train_set = Dataset(X_train_tensor, Y_train_tensor)\n",
    "\n",
    "submit_set = Dataset(X_submit_tensor, Y_submit_tensor)\n",
    "\n",
    "train_sampler = SubsetRandomSampler(train_indices)\n",
    "submit_sampler = SubsetRandomSampler(submit_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=batch_size,\n",
    "                                           sampler=train_sampler)\n",
    "submit_loader = torch.utils.data.DataLoader(submit_set, batch_size=batch_size,\n",
    "                                         sampler=submit_sampler)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "По сути следующая клетка — наше поле для экспериментов с гиперпараметрами (не считая размера батча из предыдущей).\n",
    "*ВАЖНО. В требованиях к выполнению задания была воспроизводимость результатов. Нейросеть я естественно обучал на видеокарте, причём совершенно домашней — GTX 1060 3Gb. Если на компе, на котором это задание будет проверяться не установлен пайторч нужным образом с поддержкой cuda (https://pytorch.org/get-started/locally/), либо на нём нет видеокарты с поддержкой CUDA, либо по ещё каким-то причинам этот код не захочет запускаться, этот ноутбук всегда можно запустить в google colab (https://colab.research.google.com/) и запустить его там. Видеопамяти там ещё больше, чем на моей домашней машинке, так что проблем точно не возникнет, я проверял :)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "neurons = 2048\n",
    "nn_model = nn.Sequential(\n",
    "    nn.Linear(1020, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    #nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    #nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    #nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    #nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "   #nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, neurons),\n",
    "    nn.ReLU(inplace=True),\n",
    "    nn.BatchNorm1d(neurons),\n",
    "    nn.Linear(neurons, 1)\n",
    ")\n",
    "nn_model.type(torch.cuda.FloatTensor)\n",
    "nn_model.to(device)\n",
    "\n",
    "loss = nn.L1Loss().type(torch.cuda.FloatTensor)\n",
    "\n",
    "optimizer = optim.Adam(nn_model.parameters(), lr=7e-1, weight_decay=1e-2, amsgrad = True)\n",
    "#optimizer = optim.SGD(nn_model.parameters(), lr = 0.5, momentum=0.9, nesterov=True)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=8, gamma=0.1)\n",
    "scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor = 0.1, patience = 5)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "В качестве лосс-функции берём L1loss, что представляет из себя MAE, что потребовал от нас заказчик"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average train loss: 687.680611, Average val loss: 392.035217\n",
      "Average train loss: 239.293819, Average val loss: 164964.718750\n",
      "Average train loss: 142.468666, Average val loss: 57179764.000000\n",
      "Average train loss: 56.729272, Average val loss: 4369.536133\n",
      "Average train loss: 59.517252, Average val loss: 6452.721436\n",
      "Average train loss: 66.585455, Average val loss: 27519.580078\n",
      "Average train loss: 90.525980, Average val loss: 1409.154053\n",
      "Average train loss: 27.561234, Average val loss: 70.009007\n",
      "Average train loss: 14.589830, Average val loss: 22.648419\n",
      "Average train loss: 11.699242, Average val loss: 0.315786\n",
      "Average train loss: 11.014960, Average val loss: 9.285642\n",
      "Average train loss: 10.282801, Average val loss: 16.251195\n",
      "Average train loss: 11.083506, Average val loss: 11.479354\n",
      "Average train loss: 11.069700, Average val loss: 9.969045\n",
      "Average train loss: 10.002920, Average val loss: 9.479786\n",
      "Average train loss: 10.375856, Average val loss: 9.023271\n",
      "Average train loss: 9.937660, Average val loss: 3.387107\n",
      "SUCCESS!\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, val_loader, loss, optimizer, scheduler, scheduler2, num_epochs):\n",
    "    train_loss_history = []\n",
    "    val_loss_history = []\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train() # Enter train mode\n",
    "\n",
    "        loss_accum = 0\n",
    "\n",
    "        for i_step, (x, y) in enumerate(train_loader):\n",
    "            x_gpu = x.to(device)\n",
    "            y_gpu = y.to(device)\n",
    "            prediction = model(x_gpu)\n",
    "            loss_value = loss(prediction, y_gpu)\n",
    "            optimizer.zero_grad()\n",
    "            loss_value.backward()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            loss_accum += float(loss_value)\n",
    "\n",
    "        ave_loss = loss_accum / (i_step + 1)\n",
    "\n",
    "        val_loss = compute_accuracy(model, val_loader)\n",
    "\n",
    "        if (ave_loss < 10) and (val_loss < 10):\n",
    "            print(\"Average train loss: %f, Average val loss: %f\" % (ave_loss, val_loss))\n",
    "            print(\"SUCCESS!\")\n",
    "            torch.save(model.state_dict(), \"model.pt\") #Сохраняем модельку, если точность нас устраивает\n",
    "            break #И выходим из цикла\n",
    "        scheduler.step()\n",
    "        scheduler2.step(val_loss)\n",
    "\n",
    "        train_loss_history.append(float(ave_loss))\n",
    "        val_loss_history.append(float(val_loss))\n",
    "\n",
    "        print(\"Average train loss: %f, Average val loss: %f\" % (ave_loss, val_loss))\n",
    "\n",
    "    return train_loss_history, val_loss_history\n",
    "\n",
    "def compute_accuracy(model, val_loader):\n",
    "    \"\"\"\n",
    "    Computes accuracy on the dataset wrapped in a loader\n",
    "\n",
    "    Returns: accuracy as a float value between 0 and 1\n",
    "    \"\"\"\n",
    "    model.eval() # Evaluation mode\n",
    "\n",
    "    loss_accum = 0\n",
    "\n",
    "    for i_step, (x, y) in enumerate(val_loader):\n",
    "        x_gpu = x.to(device)\n",
    "        y_gpu = y.to(device)\n",
    "        prediction = model(x_gpu)\n",
    "        loss_value = float(loss(prediction, y_gpu))\n",
    "        loss_accum += loss_value\n",
    "\n",
    "    ave_loss = loss_accum / (i_step + 1)\n",
    "\n",
    "\n",
    "    #raise Exception(\"Not implemented\")\n",
    "\n",
    "    return ave_loss\n",
    "\n",
    "#train_loss_history, val_loss_history = train_model(nn_model, train_loader, val_loader, loss, optimizer, scheduler, 200) добавим уменьшение лёрнинг рейта на плато валидационного лосса\n",
    "train_loss_history, val_loss_history = train_model(nn_model, train_loader, submit_loader, loss, optimizer, scheduler, scheduler2, 50)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Следующая строчка кода просто для внезапной очистки памяти на GPU. У меня было всего 3Gb, мне было полезно. Использовать с осторожностью)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Построим графики лосса на валидации и трейне. Удалим несколько первых элементов, ибо там лосс непомерно большой и график будет неинформативный. Индексы для удаления просто на глаз исходя из вывода сверху. Зачастую удаления первых 8 элементов достаточно."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "outputs": [],
   "source": [
    "a = train_loss_history\n",
    "b = val_loss_history\n",
    "\n",
    "del(a[0:8])\n",
    "del(b[0:8])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "x_axis = list(range(len(a)))\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(x_axis, a, label = \"train\")\n",
    "ax.plot(x_axis, b, label = \"val\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "ax.legend()\n",
    "fig.savefig(\"train_val_loss_3.pdf\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5808, 1])"
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_submit_tensor.size()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Модель натренировали, приступаем к предсказаниям. Сделаем лоадер"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[416.5745],\n        [416.7686],\n        [416.8349],\n        ...,\n        [416.1400],\n        [416.9569],\n        [416.3790]], grad_fn=<CopySlices>)"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_model.load_state_dict(torch.load(\"model_backup.pt\"))\n",
    "nn_model.eval()\n",
    "\n",
    "#batch_size = len(Y_submit_tensor)\n",
    "batch_size = 1\n",
    "predict_indices = list(range(len(Y_submit_tensor)))\n",
    "\n",
    "predict_set = Dataset(X_submit_tensor, Y_submit_tensor)\n",
    "\n",
    "predict_sampler = Sampler(submit_indices)\n",
    "\n",
    "predict_loader = torch.utils.data.DataLoader(predict_set, batch_size=batch_size,\n",
    "                                            sampler=submit_sampler)\n",
    "shape = (5808, 1,)\n",
    "pred_tensor = torch.zeros(shape)\n",
    "#pred_tensor_gpu = pred_tensor.to(device)\n",
    "\n",
    "for i_step, (x, y) in enumerate(predict_loader):\n",
    "    prediction = nn_model(x)\n",
    "    pred_tensor[i_step, 0] = prediction\n",
    "\n",
    "pred_tensor"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "а вот и наши долгожданные предикты, теперь добавим их в датафрейм предикта и напечтаем в csv. Посчитаем MAE и пойдём довольные писать отчёт"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                     score  predictions  absolute error\nDate                                                   \n2018-05-04 00:05:00    420   416.574493        3.425507\n2018-05-04 01:05:00    420   416.768585        3.231415\n2018-05-04 02:05:00    420   416.834900        3.165100\n2018-05-04 03:05:00    420   416.286407        3.713593\n2018-05-04 04:05:00    420   416.326874        3.673126",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>score</th>\n      <th>predictions</th>\n      <th>absolute error</th>\n    </tr>\n    <tr>\n      <th>Date</th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>2018-05-04 00:05:00</th>\n      <td>420</td>\n      <td>416.574493</td>\n      <td>3.425507</td>\n    </tr>\n    <tr>\n      <th>2018-05-04 01:05:00</th>\n      <td>420</td>\n      <td>416.768585</td>\n      <td>3.231415</td>\n    </tr>\n    <tr>\n      <th>2018-05-04 02:05:00</th>\n      <td>420</td>\n      <td>416.834900</td>\n      <td>3.165100</td>\n    </tr>\n    <tr>\n      <th>2018-05-04 03:05:00</th>\n      <td>420</td>\n      <td>416.286407</td>\n      <td>3.713593</td>\n    </tr>\n    <tr>\n      <th>2018-05-04 04:05:00</th>\n      <td>420</td>\n      <td>416.326874</td>\n      <td>3.673126</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_array = pred_tensor.detach().numpy()\n",
    "Y_submit[\"predictions\"] = pred_array\n",
    "\n",
    "Y_submit[\"absolute error\"] = abs(Y_submit.score - Y_submit.predictions)\n",
    "Y_submit.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE:  3.3932678548429296\n"
     ]
    }
   ],
   "source": [
    "print(\"MAE: \", Y_submit[\"absolute error\"].mean())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "Y_submit.to_csv(\"Y_sub_pred.csv\", index = True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}